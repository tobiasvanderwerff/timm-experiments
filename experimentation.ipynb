{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/tobias/conditional_batchnorm/env/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ubuntu/tobias/conditional_batchnorm/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from layers import ConditionalBatchNorm2d\n",
    "from utils import create_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet10t.c3_in1k\n",
      "resnet14t.c3_in1k\n",
      "resnet18.a1_in1k\n",
      "resnet18.a2_in1k\n",
      "resnet18.a3_in1k\n",
      "resnet18.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet18.fb_swsl_ig1b_ft_in1k\n",
      "resnet18.gluon_in1k\n",
      "resnet18.tv_in1k\n",
      "resnet18d.ra2_in1k\n",
      "resnet26.bt_in1k\n",
      "resnet26d.bt_in1k\n",
      "resnet26t.ra2_in1k\n",
      "resnet32ts.ra2_in1k\n",
      "resnet33ts.ra2_in1k\n",
      "resnet34.a1_in1k\n",
      "resnet34.a2_in1k\n",
      "resnet34.a3_in1k\n",
      "resnet34.bt_in1k\n",
      "resnet34.gluon_in1k\n",
      "resnet34.tv_in1k\n",
      "resnet34d.ra2_in1k\n",
      "resnet50.a1_in1k\n",
      "resnet50.a1h_in1k\n",
      "resnet50.a2_in1k\n",
      "resnet50.a3_in1k\n",
      "resnet50.am_in1k\n",
      "resnet50.b1k_in1k\n",
      "resnet50.b2k_in1k\n",
      "resnet50.bt_in1k\n",
      "resnet50.c1_in1k\n",
      "resnet50.c2_in1k\n",
      "resnet50.d_in1k\n",
      "resnet50.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet50.fb_swsl_ig1b_ft_in1k\n",
      "resnet50.gluon_in1k\n",
      "resnet50.ra_in1k\n",
      "resnet50.ram_in1k\n",
      "resnet50.tv2_in1k\n",
      "resnet50.tv_in1k\n",
      "resnet50_gn.a1h_in1k\n",
      "resnet50c.gluon_in1k\n",
      "resnet50d.a1_in1k\n",
      "resnet50d.a2_in1k\n",
      "resnet50d.a3_in1k\n",
      "resnet50d.gluon_in1k\n",
      "resnet50d.ra2_in1k\n",
      "resnet50s.gluon_in1k\n",
      "resnet51q.ra2_in1k\n",
      "resnet61q.ra2_in1k\n",
      "resnet101.a1_in1k\n",
      "resnet101.a1h_in1k\n",
      "resnet101.a2_in1k\n",
      "resnet101.a3_in1k\n",
      "resnet101.gluon_in1k\n",
      "resnet101.tv2_in1k\n",
      "resnet101.tv_in1k\n",
      "resnet101c.gluon_in1k\n",
      "resnet101d.gluon_in1k\n",
      "resnet101d.ra2_in1k\n",
      "resnet101s.gluon_in1k\n",
      "resnet152.a1_in1k\n",
      "resnet152.a1h_in1k\n",
      "resnet152.a2_in1k\n",
      "resnet152.a3_in1k\n",
      "resnet152.gluon_in1k\n",
      "resnet152.tv2_in1k\n",
      "resnet152.tv_in1k\n",
      "resnet152c.gluon_in1k\n",
      "resnet152d.gluon_in1k\n",
      "resnet152d.ra2_in1k\n",
      "resnet152s.gluon_in1k\n",
      "resnet200d.ra2_in1k\n",
      "resnetaa50.a1h_in1k\n",
      "resnetaa50d.d_in12k\n",
      "resnetaa50d.sw_in12k\n",
      "resnetaa50d.sw_in12k_ft_in1k\n",
      "resnetaa101d.sw_in12k\n",
      "resnetaa101d.sw_in12k_ft_in1k\n",
      "resnetblur50.bt_in1k\n",
      "resnetrs50.tf_in1k\n",
      "resnetrs101.tf_in1k\n",
      "resnetrs152.tf_in1k\n",
      "resnetrs200.tf_in1k\n",
      "resnetrs270.tf_in1k\n",
      "resnetrs350.tf_in1k\n",
      "resnetrs420.tf_in1k\n",
      "resnetv2_50.a1h_in1k\n",
      "resnetv2_50d_evos.ah_in1k\n",
      "resnetv2_50d_gn.ah_in1k\n",
      "resnetv2_50x1_bit.goog_distilled_in1k\n",
      "resnetv2_50x1_bit.goog_in21k\n",
      "resnetv2_50x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_50x3_bit.goog_in21k\n",
      "resnetv2_50x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101.a1h_in1k\n",
      "resnetv2_101x1_bit.goog_in21k\n",
      "resnetv2_101x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101x3_bit.goog_in21k\n",
      "resnetv2_101x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_in21k\n",
      "resnetv2_152x2_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384\n",
      "resnetv2_152x4_bit.goog_in21k\n",
      "resnetv2_152x4_bit.goog_in21k_ft_in1k\n"
     ]
    }
   ],
   "source": [
    "model_names = timm.list_models(\"resnet*\", pretrained=True)\n",
    "print(\"\\n\".join(model_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: timm has multiple versions of the resnet50 model, with different\n",
    "# suffixes (e.g. resnet50.a1_in1k, resnet50.d_in1k, etc.). Figure out what this\n",
    "# means and how to choose the right model.\n",
    "\n",
    "model = timm.create_model('resnet18', pretrained=True, num_classes=37)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': (3, 224, 224),\n",
       " 'interpolation': 'bicubic',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'crop_pct': 0.95,\n",
       " 'crop_mode': 'center',\n",
       " 'is_training': True}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create model-specific transform\n",
    "data_config = timm.data.resolve_data_config(model.pretrained_cfg)\n",
    "data_config.update({\n",
    "    \"is_training\": True\n",
    "})\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.data.create_transform(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=235, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform = timm.data.create_transform(**data_config)\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transform = timm.data.create_transform(**data_config)\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': (3, 224, 224),\n",
       " 'interpolation': 'bicubic',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'crop_pct': 0.95,\n",
       " 'crop_mode': 'center'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze all layers\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze the final layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\"\"\"\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        print(m.bias.shape)\n",
    "\"\"\"\n",
    "\n",
    "# Add conditional batch norm layers\n",
    "ConditionalBatchNorm2d.replace_bn2d(model)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "percent_trainable = num_trainable_params / num_params * 100\n",
    "\n",
    "print(f\"Number of parameters: {num_params:,}\")\n",
    "print(f\"Number of trainable parameters: {num_trainable_params:,} ({percent_trainable:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data\n",
    "dataset_train = torchvision.datasets.OxfordIIITPet(\"data\", split=\"trainval\", download=True)\n",
    "dataset_test = torchvision.datasets.OxfordIIITPet(\"data\", split=\"test\")\n",
    "\n",
    "# from timm.data import create_dataset\n",
    "# ds = create_dataset(\"torch/oxford_iiit_pet\", root=\"data\", split=\"trainval\", download=True)\n",
    "\n",
    "# Let's now make a subset of the training dataset with N images per class.\n",
    "dataset_train = create_subset(dataset_train, n_img_per_class=10, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_classifier().training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=333x500>, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
